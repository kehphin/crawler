Overall, this project wasn’t too bad. We met up twice for about 2 hours each, and then worked on it independently for a couple more hours for the rest of the program. We approached this project in multiple steps.

We first researched how to implement HTTP requests and responses using only sockets. Once we had our boilerplate socket code down, we tackled the Login aspect. This was probably the most challenging part due to both the redirect and ensuring that the cookie params were used properly (CSRF). 

Then, once we logged in, we proceeded to tackle the actual web crawler portion. Our high level implementation was: scrape all links on a page using an HTML parser, toss out any links that didn’t have a path of /fakebook, and add them to a queue. From there, it was simply a matter of repeatedly popping off a path from the list, visiting the webpage corresponding to the path, and searching for additional links and secret flags. Of course, to prevent loops, we also kept a dictionary of the paths that we had already visited, so if we encountered it again, we would not visit it. This dictionary proved useful because it offers O(1) lookup time for paths, and there were certain points where the dictionary was over 50000 entries deep. This allowed for our program to run more efficiently. Of course, we were limited by the time it took for the HTTP requests. If we had more time, a multithreaded implementation could have rewarded us with a significant speedup in crawl time. We decided to go for an iterative solution rather than a recursive crawler, because we knew that recursing 10000+ levels deep wouldn’t be practical and would result in a stack overflow.

After this, it was a matter of dealing with a couple of edge cases such as 500 errors and 404 errors. Solving these edge cases allowed us to crawl Fakebook and obtain all the keys in around 5 minutes.